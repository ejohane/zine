{"id":"zine-n9v","title":"Subscriptions Spec Review: Address Gaps and Issues","description":"This epic captures all findings from a thorough review of features/subscriptions/spec.md. The spec is comprehensive (~9500 lines) but has several gaps, inconsistencies, and areas needing clarification before implementation can proceed smoothly. Priority: P0 = blocking implementation, P1 = should fix before implementation, P2 = nice to have.","status":"closed","priority":1,"issue_type":"epic","created_at":"2025-12-16T19:34:13.078005-06:00","updated_at":"2025-12-16T20:07:20.08125-06:00","closed_at":"2025-12-16T20:07:20.08125-06:00"}
{"id":"zine-n9v.1","title":"Add missing provider_items_seen table schema to Appendix C","description":"## Problem\nThe `provider_items_seen` table is extensively referenced in §1.3 (Table Purpose Reference) with detailed queries and example usage, but its CREATE TABLE schema is missing from Appendix C (Database Migration).\n\n## Context\n- §1.3 describes the table's purpose: idempotency layer to prevent duplicate ingestion\n- Shows columns: `user_id`, `provider`, `provider_item_id`, `source_id`, `first_seen_at`\n- Unique constraint: `(user_id, provider, provider_item_id)`\n- The ingestion code in §4.3 references this table\n\n## Solution\nAdd the following to Appendix C after the `subscription_items` table:\n\n```sql\nCREATE TABLE provider_items_seen (\n  id TEXT PRIMARY KEY,\n  user_id TEXT NOT NULL REFERENCES users(id) ON DELETE CASCADE,\n  provider TEXT NOT NULL,\n  provider_item_id TEXT NOT NULL,\n  source_id TEXT,  -- subscription_id that first ingested this\n  user_item_id TEXT REFERENCES user_items(id),\n  first_seen_at INTEGER NOT NULL DEFAULT (unixepoch() * 1000),\n  UNIQUE(user_id, provider, provider_item_id)\n);\n\nCREATE INDEX idx_provider_items_seen_lookup \n  ON provider_items_seen(user_id, provider, provider_item_id);\n```\n\n## Files\n- features/subscriptions/spec.md (Appendix C, around line 9408)","status":"closed","priority":0,"issue_type":"task","created_at":"2025-12-16T19:34:13.155594-06:00","updated_at":"2025-12-16T19:39:36.644099-06:00","closed_at":"2025-12-16T19:39:36.644099-06:00","dependencies":[{"issue_id":"zine-n9v.1","depends_on_id":"zine-n9v","type":"parent-child","created_at":"2025-12-16T19:34:13.155936-06:00","created_by":"daemon"}]}
{"id":"zine-n9v.10","title":"Add pagination to subscriptions.list API endpoint","description":"## Problem\nThe `subscriptions.list` endpoint (§5.1 line 3854) returns all subscriptions without pagination. Users with many subscriptions would get unbounded response sizes.\n\n## Context\n- Active users might have 50+ subscriptions\n- Each subscription object includes metadata (name, description, imageUrl)\n- No `limit` or `cursor` parameters shown in the input schema\n- Mobile app list views benefit from pagination for performance\n\n## Solution\nUpdate §5.1 to include pagination:\n\n```typescript\nlist: protectedProcedure\n  .input(\n    z.object({\n      provider: ProviderSchema.optional(),\n      status: SubscriptionStatusSchema.optional(),\n      // Pagination\n      limit: z.number().min(1).max(100).default(50),\n      cursor: z.string().optional(), // subscription ID for cursor-based pagination\n    }).optional()\n  )\n  .query(async ({ ctx, input }) =\u003e {\n    const { provider, status, limit = 50, cursor } = input || {};\n    \n    let whereClause = eq(subscriptions.userId, ctx.userId);\n    \n    if (provider) {\n      whereClause = and(whereClause, eq(subscriptions.provider, provider));\n    }\n    if (status) {\n      whereClause = and(whereClause, eq(subscriptions.status, status));\n    }\n    if (cursor) {\n      whereClause = and(whereClause, gt(subscriptions.id, cursor));\n    }\n    \n    const results = await ctx.db.query.subscriptions.findMany({\n      where: whereClause,\n      orderBy: [asc(subscriptions.id)],\n      limit: limit + 1, // Fetch one extra to detect hasMore\n    });\n    \n    const hasMore = results.length \u003e limit;\n    const items = hasMore ? results.slice(0, -1) : results;\n    const nextCursor = hasMore ? items[items.length - 1].id : null;\n    \n    return {\n      items,\n      nextCursor,\n      hasMore,\n    };\n  }),\n```\n\nAlso update the mobile hook in §6.6.4 to use infinite query.\n\n## Files\n- features/subscriptions/spec.md (§5.1 around line 3854, §6.6.4)","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-16T19:34:13.822511-06:00","updated_at":"2025-12-16T19:44:11.202671-06:00","closed_at":"2025-12-16T19:44:11.202671-06:00","dependencies":[{"issue_id":"zine-n9v.10","depends_on_id":"zine-n9v","type":"parent-child","created_at":"2025-12-16T19:34:13.822843-06:00","created_by":"daemon"}]}
{"id":"zine-n9v.11","title":"Add providerChannelId format validation (YouTube: UC prefix)","description":"## Problem\nThe `subscriptions.add` mutation accepts any string for `providerChannelId` without validation. Invalid IDs would fail silently during polling.\n\n## Context\n- YouTube channel IDs have a specific format: UC prefix + 22 base64 characters (e.g., `UCBJycsmduvYEL83R_U4JriQ`)\n- Spotify show IDs are 22 alphanumeric characters (e.g., `5rgumWEx4FsqIY8e1wJNAk`)\n- Invalid IDs would be stored and polled, wasting API quota\n- Errors only surface during polling, not at subscription time\n\n## Solution\nAdd format validation to §5.1 `subscriptions.add`:\n\n```typescript\n// packages/shared/src/schemas/index.ts\n\nexport const YouTubeChannelIdSchema = z.string()\n  .regex(/^UC[a-zA-Z0-9_-]{22}$/, 'Invalid YouTube channel ID format');\n\nexport const SpotifyShowIdSchema = z.string()\n  .regex(/^[a-zA-Z0-9]{22}$/, 'Invalid Spotify show ID format');\n\nexport const ProviderChannelIdSchema = z.string().min(1);\n\n// In subscriptions router\nadd: protectedProcedure\n  .input(\n    z.object({\n      provider: ProviderSchema,\n      providerChannelId: z.string().min(1),\n      name: z.string().optional(),\n      imageUrl: z.string().url().optional(),\n    }).superRefine((data, ctx) =\u003e {\n      if (data.provider === 'YOUTUBE') {\n        const result = YouTubeChannelIdSchema.safeParse(data.providerChannelId);\n        if (!result.success) {\n          ctx.addIssue({\n            code: z.ZodIssueCode.custom,\n            message: 'Invalid YouTube channel ID format. Expected UC + 22 characters.',\n            path: ['providerChannelId'],\n          });\n        }\n      } else if (data.provider === 'SPOTIFY') {\n        const result = SpotifyShowIdSchema.safeParse(data.providerChannelId);\n        if (!result.success) {\n          ctx.addIssue({\n            code: z.ZodIssueCode.custom,\n            message: 'Invalid Spotify show ID format. Expected 22 alphanumeric characters.',\n            path: ['providerChannelId'],\n          });\n        }\n      }\n    })\n  )\n```\n\nNote: This is P2 priority since discovery flows will typically provide valid IDs. Validation is a defense-in-depth measure.\n\n## Files\n- features/subscriptions/spec.md (§5.1 around line 3864)","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-16T19:34:13.895119-06:00","updated_at":"2025-12-16T20:07:02.427936-06:00","closed_at":"2025-12-16T20:07:02.427936-06:00","dependencies":[{"issue_id":"zine-n9v.11","depends_on_id":"zine-n9v","type":"parent-child","created_at":"2025-12-16T19:34:13.895441-06:00","created_by":"daemon"}]}
{"id":"zine-n9v.12","title":"Add rate limiting to OAuth callback endpoint","description":"## Problem\nThe OAuth `callback` endpoint (§2.2) has no rate limiting, allowing attackers to brute force the state parameter to hijack OAuth flows.\n\n## Context\n- State is a UUID (122 bits of entropy) so brute force is impractical\n- But rate limiting is still defense-in-depth\n- The endpoint does DB writes (token storage) so abuse could cause resource exhaustion\n- §7.3 references rate limiting for provider APIs but not for our own endpoints\n\n## Solution\nAdd rate limiting section §2.2.1 \"OAuth Endpoint Protection\":\n\n```typescript\n// apps/worker/src/middleware/rate-limit.ts\n\ninterface RateLimitConfig {\n  windowMs: number;\n  maxRequests: number;\n  keyPrefix: string;\n}\n\nconst OAUTH_CALLBACK_LIMIT: RateLimitConfig = {\n  windowMs: 60 * 1000, // 1 minute window\n  maxRequests: 10,     // Max 10 attempts per minute\n  keyPrefix: 'ratelimit:oauth:callback',\n};\n\nexport async function checkRateLimit(\n  env: Env,\n  config: RateLimitConfig,\n  identifier: string\n): Promise\u003c{ allowed: boolean; remaining: number; resetAt: number }\u003e {\n  const key = `${config.keyPrefix}:${identifier}`;\n  const now = Date.now();\n  const windowStart = now - config.windowMs;\n  \n  // ... sliding window implementation similar to §3.7.4 ...\n}\n\n// In connections router\ncallback: protectedProcedure\n  .input(/* ... */)\n  .mutation(async ({ ctx, input }) =\u003e {\n    // Rate limit by user ID (authenticated) and state (unauthenticated attempts)\n    const userLimit = await checkRateLimit(ctx.env, OAUTH_CALLBACK_LIMIT, ctx.userId);\n    if (!userLimit.allowed) {\n      throw new TRPCError({\n        code: 'TOO_MANY_REQUESTS',\n        message: `Too many OAuth attempts. Try again in ${Math.ceil((userLimit.resetAt - Date.now()) / 1000)} seconds.`,\n      });\n    }\n    \n    // ... existing callback logic ...\n  }),\n```\n\nConsider also rate limiting the `registerState` endpoint.\n\n## Files\n- features/subscriptions/spec.md (new section §2.2.1)","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-16T19:34:13.967991-06:00","updated_at":"2025-12-16T19:44:54.0798-06:00","closed_at":"2025-12-16T19:44:54.0798-06:00","dependencies":[{"issue_id":"zine-n9v.12","depends_on_id":"zine-n9v","type":"parent-child","created_at":"2025-12-16T19:34:13.968307-06:00","created_by":"daemon"}]}
{"id":"zine-n9v.13","title":"Add custom URL scheme registration for Android in app.json","description":"## Problem\nThe `app.json` configuration (§6.5.1) shows intent filters for HTTPS Universal Links but doesn't explicitly register the custom `zine://` URL scheme for Android.\n\n## Context\n- Line 4619 shows `\"scheme\": \"zine\"` in expo config which handles iOS\n- Lines 4627-4647 show Android intent filters only for HTTPS links\n- Custom schemes need explicit Android configuration\n- OAuth redirect uses `zine://oauth/callback` (line 4661)\n- Without explicit config, Android deep links may not work\n\n## Solution\nUpdate §6.5.1 app.json to include custom scheme for Android:\n\n```json\n{\n  \"expo\": {\n    \"scheme\": \"zine\",\n    \"android\": {\n      \"package\": \"app.zine.mobile\",\n      \"intentFilters\": [\n        {\n          \"action\": \"VIEW\",\n          \"autoVerify\": true,\n          \"data\": [\n            {\n              \"scheme\": \"https\",\n              \"host\": \"zine.app\",\n              \"pathPrefix\": \"/oauth/callback\"\n            }\n          ],\n          \"category\": [\"BROWSABLE\", \"DEFAULT\"]\n        },\n        {\n          \"action\": \"VIEW\",\n          \"data\": [\n            {\n              \"scheme\": \"zine\",\n              \"host\": \"oauth\",\n              \"pathPrefix\": \"/callback\"\n            }\n          ],\n          \"category\": [\"BROWSABLE\", \"DEFAULT\"]\n        }\n      ]\n    }\n  }\n}\n```\n\nNote: The `scheme` at root level handles this in Expo, but explicit intent filter ensures it works in all Android build configurations.\n\n## Files\n- features/subscriptions/spec.md (§6.5.1 around line 4627)","status":"closed","priority":0,"issue_type":"task","created_at":"2025-12-16T19:34:14.042047-06:00","updated_at":"2025-12-16T19:41:04.367827-06:00","closed_at":"2025-12-16T19:41:04.367827-06:00","dependencies":[{"issue_id":"zine-n9v.13","depends_on_id":"zine-n9v","type":"parent-child","created_at":"2025-12-16T19:34:14.042342-06:00","created_by":"daemon"}]}
{"id":"zine-n9v.14","title":"Extend OAuth state expiry from 10 to 30 minutes","description":"## Problem\nThe OAuth state expiry of 10 minutes (§7.2 line 6934) may be too short for users who need to log into their Google/Spotify account or have slow connections.\n\n## Context\n- User clicks \"Connect YouTube\"\n- State registered with 10 min TTL\n- User redirected to Google\n- User needs to: log in (if not already), review permissions, click authorize\n- On slow connections or complex account situations (2FA, account selection), 10 min may not be enough\n- State expiry error is confusing to users\n\n## Solution\nIncrease TTL to 30 minutes in §7.2:\n\n```typescript\nasync function generateOAuthState(userId: string, env: Env): Promise\u003cstring\u003e {\n  const state = crypto.randomUUID();\n  await env.KV.put(`oauth:state:${state}`, userId, { \n    expirationTtl: 1800  // 30 minutes (was 600)\n  });\n  return state;\n}\n```\n\nAlso update §2.2 `registerState` endpoint comment (line 3655) and mobile code (§5.2).\n\nRationale: 30 minutes is still short enough to limit replay window but gives users reasonable time to complete OAuth flow.\n\n## Files\n- features/subscriptions/spec.md (§7.2 line 6934, §2.2 line 3655)","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-16T19:34:14.11831-06:00","updated_at":"2025-12-16T20:07:03.471978-06:00","closed_at":"2025-12-16T20:07:03.471978-06:00","dependencies":[{"issue_id":"zine-n9v.14","depends_on_id":"zine-n9v","type":"parent-child","created_at":"2025-12-16T19:34:14.118666-06:00","created_by":"daemon"}]}
{"id":"zine-n9v.15","title":"Use tRPC utils for query invalidation instead of raw query keys","description":"## Problem\nThe optimistic update code in §6.6.4 uses raw TanStack Query keys that match tRPC's internal format. This is fragile and may break with tRPC version updates.\n\n## Context\n- Line 6042-6044 uses: `[['subscriptions', 'list'], { input: {}, type: 'query' }]`\n- This is tRPC's internal cache key format, not a public API\n- tRPC provides `utils.client` for mutations and `utils.subscriptions.list.invalidate()` for cache operations\n- Should use tRPC's utility methods instead\n\n## Solution\nUpdate §6.6.4 to use tRPC's cache utilities:\n\n```typescript\n// Instead of:\nqueryClient.setQueryData\u003cSubscription[]\u003e(\n  [['subscriptions', 'list'], { input: {}, type: 'query' }],\n  (old) =\u003e ...\n);\n\n// Use tRPC's getQueryKey helper:\nimport { getQueryKey } from '@trpc/react-query';\n\nconst queryKey = getQueryKey(trpc.subscriptions.list, {}, 'query');\nqueryClient.setQueryData\u003cSubscription[]\u003e(queryKey, (old) =\u003e ...);\n\n// Or better, use utils.setData (if available in your tRPC version):\nutils.subscriptions.list.setData({}, (old) =\u003e \n  old ? [...old, optimisticSub] : [optimisticSub]\n);\n```\n\nAlso update the hook to use the documented tRPC patterns.\n\n## Files\n- features/subscriptions/spec.md (§6.6.4 lines 6042-6051, 6072-6074)","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-16T19:34:14.191834-06:00","updated_at":"2025-12-16T19:57:57.110637-06:00","closed_at":"2025-12-16T19:57:57.110637-06:00","dependencies":[{"issue_id":"zine-n9v.15","depends_on_id":"zine-n9v","type":"parent-child","created_at":"2025-12-16T19:34:14.192185-06:00","created_by":"daemon"}]}
{"id":"zine-n9v.16","title":"Add TypeScript type definitions for YouTubeVideo and SpotifyEpisode","description":"## Problem\nThe item transformation functions (§4.2) reference `YouTubeVideo` and `SpotifyEpisode` types but these aren't defined in the spec.\n\n## Context\n- Line 3239: `transformYouTubeVideo(playlistItem: youtube_v3.Schema$PlaylistItem)`\n- Line 3267: `transformSpotifyEpisode(episode: SimplifiedEpisode, ...)`\n- `youtube_v3.Schema$PlaylistItem` comes from googleapis types\n- `SimplifiedEpisode` comes from @spotify/web-api-ts-sdk\n- But §4.3 line 3318 uses `YouTubeVideo` and `SpotifyEpisode` without definition\n- Creates confusion about whether these are SDK types or custom types\n\n## Solution\nAdd a types section §4.1.1 \"Provider Item Types\":\n\n```typescript\n// packages/shared/src/types/providers.ts\n\nimport type { youtube_v3 } from 'googleapis';\nimport type { SimplifiedEpisode, Episode } from '@spotify/web-api-ts-sdk';\n\n/**\n * YouTube video as returned from playlistItems.list\n */\nexport type YouTubeVideo = youtube_v3.Schema$PlaylistItem;\n\n/**\n * Spotify episode as returned from shows.getShowEpisodes\n */\nexport type SpotifyEpisode = SimplifiedEpisode;\n\n/**\n * Union type for any provider item\n */\nexport type ProviderItem = YouTubeVideo | SpotifyEpisode;\n\n/**\n * Type guard for YouTube items\n */\nexport function isYouTubeVideo(item: ProviderItem): item is YouTubeVideo {\n  return 'snippet' in item \u0026\u0026 'contentDetails' in item;\n}\n\n/**\n * Type guard for Spotify episodes\n */\nexport function isSpotifyEpisode(item: ProviderItem): item is SpotifyEpisode {\n  return 'release_date' in item \u0026\u0026 'duration_ms' in item;\n}\n```\n\nUpdate §4.3 to import and use these types consistently.\n\n## Files\n- features/subscriptions/spec.md (new section §4.1.1, update §4.3)","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-16T19:34:14.26559-06:00","updated_at":"2025-12-16T19:57:58.335918-06:00","closed_at":"2025-12-16T19:57:58.335918-06:00","dependencies":[{"issue_id":"zine-n9v.16","depends_on_id":"zine-n9v","type":"parent-child","created_at":"2025-12-16T19:34:14.26591-06:00","created_by":"daemon"}]}
{"id":"zine-n9v.17","title":"Consolidate duplicated hexToBytes/bytesToHex helper functions","description":"## Problem\nThe `hexToBytes` and `bytesToHex` helper functions are defined in both §7.1 (lines 6824-6839) and §7.6.3 (lines 7495-7507), creating code duplication.\n\n## Context\n- First definition in §7.1 \"Token Storage\" encryption example\n- Second definition in §7.6.3 \"Versioned Encryption Implementation\"\n- Identical implementations\n- Should be consolidated into a shared utility\n\n## Solution\n1. Remove the duplicate from §7.6.3\n2. Add a note in §7.1 that these utilities should be in a shared module:\n\n```typescript\n// apps/worker/src/lib/encoding.ts\n\n/**\n * Convert a hex string to Uint8Array.\n * Used for encryption key handling and ciphertext parsing.\n */\nexport function hexToBytes(hex: string): Uint8Array {\n  const bytes = new Uint8Array(hex.length / 2);\n  for (let i = 0; i \u003c hex.length; i += 2) {\n    bytes[i / 2] = parseInt(hex.substring(i, i + 2), 16);\n  }\n  return bytes;\n}\n\n/**\n * Convert Uint8Array to hex string.\n * Used for encoding IVs, auth tags, and ciphertext.\n */\nexport function bytesToHex(bytes: Uint8Array): string {\n  return Array.from(bytes)\n    .map((b) =\u003e b.toString(16).padStart(2, '0'))\n    .join('');\n}\n```\n\nThen update §7.6.3 to import from this module instead of redefining.\n\n## Files\n- features/subscriptions/spec.md (§7.1 lines 6824-6839, §7.6.3 lines 7495-7507)","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-16T19:34:14.338999-06:00","updated_at":"2025-12-16T20:07:04.639847-06:00","closed_at":"2025-12-16T20:07:04.639847-06:00","dependencies":[{"issue_id":"zine-n9v.17","depends_on_id":"zine-n9v","type":"parent-child","created_at":"2025-12-16T19:34:14.339316-06:00","created_by":"daemon"}]}
{"id":"zine-n9v.18","title":"Add sequence diagram for polling → ingestion → inbox flow","description":"## Problem\nThe spec includes a sequence diagram for OAuth (§2.2) but lacks one for the polling → ingestion → inbox flow, which is the core subscription functionality.\n\n## Context\n- §4.1 has a flow diagram but it's ASCII boxes, not a sequence diagram\n- The interaction between cron trigger, poll scheduler, provider fetcher, ingestion processor, and database is complex\n- A sequence diagram would clarify timing and responsibilities\n- Helpful for implementers to understand the full data flow\n\n## Solution\nAdd a sequence diagram in §4.1 after the flow diagram:\n\n```\n┌───────────┐     ┌───────────┐     ┌──────────┐     ┌──────────┐     ┌────┐\n│   Cron    │     │  Poll     │     │ Provider │     │ Ingestion│     │ D1 │\n│  Trigger  │     │ Scheduler │     │ Fetcher  │     │ Processor│     │    │\n└─────┬─────┘     └─────┬─────┘     └────┬─────┘     └────┬─────┘     └──┬─┘\n      │                 │                │                │              │\n      │  1. trigger     │                │                │              │\n      │────────────────\u003e│                │                │              │\n      │                 │                │                │              │\n      │                 │ 2. query due   │                │              │\n      │                 │────────────────────────────────────────────────\u003e│\n      │                 │                │                │              │\n      │                 │ 3. subscriptions                │              │\n      │                 │\u003c────────────────────────────────────────────────│\n      │                 │                │                │              │\n      │                 │ 4. for each subscription:       │              │\n      │                 │──────────────\u003e│                │              │\n      │                 │                │                │              │\n      │                 │                │ 5. get token   │              │\n      │                 │                │────────────────────────────────\u003e│\n      │                 │                │                │              │\n      │                 │                │ 6. fetch items │              │\n      │                 │                │──────────────\u003e │              │\n      │                 │                │  (YouTube/     │              │\n      │                 │                │   Spotify API) │              │\n      │                 │                │                │              │\n      │                 │                │ 7. items       │              │\n      │                 │                │\u003c────────────── │              │\n      │                 │                │                │              │\n      │                 │                │ 8. for each item:              │\n      │                 │                │───────────────\u003e│              │\n      │                 │                │                │              │\n      │                 │                │                │ 9. check seen│\n      │                 │                │                │─────────────\u003e│\n      │                 │                │                │              │\n      │                 │                │                │ 10. if new:  │\n      │                 │                │                │  insert item,│\n      │                 │                │                │  user_item,  │\n      │                 │                │                │  mark seen   │\n      │                 │                │                │─────────────\u003e│\n      │                 │                │                │              │\n      │                 │ 11. update lastPolledAt         │              │\n      │                 │────────────────────────────────────────────────\u003e│\n      │                 │                │                │              │\n```\n\n## Files\n- features/subscriptions/spec.md (§4.1 after line 3229)","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-16T19:34:14.415323-06:00","updated_at":"2025-12-16T20:07:05.608162-06:00","closed_at":"2025-12-16T20:07:05.608162-06:00","dependencies":[{"issue_id":"zine-n9v.18","depends_on_id":"zine-n9v","type":"parent-child","created_at":"2025-12-16T19:34:14.415678-06:00","created_by":"daemon"}]}
{"id":"zine-n9v.19","title":"Add reference to local-first sync architecture in offline handling section","description":"## Problem\nThe offline handling section (§6.6) describes a custom offline queue implementation but doesn't reference the broader local-first sync architecture documented in the project.\n\n## Context\n- docs/zine-sync-local-first.md likely describes the app's sync strategy\n- §6.6 implements its own offline queue with AsyncStorage\n- May duplicate or conflict with existing sync infrastructure\n- Implementers should understand how subscriptions offline handling fits into the larger picture\n\n## Solution\nAdd a note at the beginning of §6.6:\n\n```markdown\n### 6.6 Offline Handling\n\n\u003e **Architecture Note**: This section describes offline handling specific to \n\u003e subscription mutations. For the broader local-first sync architecture used\n\u003e by Zine, see `docs/zine-sync-local-first.md`. The subscription offline queue\n\u003e described here is designed to integrate with that architecture.\n\u003e \n\u003e Key integration points:\n\u003e - Subscription list queries use the same cache/sync strategy as other data\n\u003e - Offline mutations queue independently since they require server-side OAuth\n\u003e - Sync recovery (§6.6.9) coordinates with the app's general sync mechanism\n\nMobile apps must gracefully handle network connectivity issues...\n```\n\nAlso review if the custom AsyncStorage queue could be replaced by or unified with the existing sync infrastructure.\n\n## Files\n- features/subscriptions/spec.md (§6.6 around line 5517)","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-16T19:34:14.49404-06:00","updated_at":"2025-12-16T20:07:06.638032-06:00","closed_at":"2025-12-16T20:07:06.638032-06:00","dependencies":[{"issue_id":"zine-n9v.19","depends_on_id":"zine-n9v","type":"parent-child","created_at":"2025-12-16T19:34:14.494384-06:00","created_by":"daemon"}]}
{"id":"zine-n9v.2","title":"Add items table schema to spec (or reference existing schema)","description":"## Problem\nThe spec references a canonical `items` table throughout (§1.2 entity diagram, §4.2 transformation, §4.3 ingestion) but never defines its schema. Implementers won't know the column structure.\n\n## Context\n- §4.2 shows `transformYouTubeVideo` and `transformSpotifyEpisode` returning a `NewItem` object with fields: `id`, `contentType`, `provider`, `providerId`, `canonicalUrl`, `title`, `description`, `creator`, `creatorUrl`, `thumbnailUrl`, `durationSeconds`, `publishedAt`, `createdAt`\n- Appendix D shows provider response shapes that map to these fields\n- The table should support multiple content types (VIDEO, PODCAST, ARTICLE, POST)\n\n## Solution\nEither:\n1. Add the schema to Appendix C if it's new\n2. Add a note referencing the existing schema location in the codebase (apps/worker/src/db/schema.ts)\n\nSuggested schema if new:\n```sql\nCREATE TABLE items (\n  id TEXT PRIMARY KEY,\n  content_type TEXT NOT NULL,  -- VIDEO, PODCAST, ARTICLE, POST\n  provider TEXT NOT NULL,       -- YOUTUBE, SPOTIFY, RSS, etc.\n  provider_id TEXT NOT NULL,    -- YouTube video ID, Spotify episode ID\n  canonical_url TEXT NOT NULL,\n  title TEXT NOT NULL,\n  description TEXT,\n  creator TEXT,\n  creator_url TEXT,\n  thumbnail_url TEXT,\n  duration_seconds INTEGER,\n  published_at INTEGER,\n  created_at INTEGER NOT NULL DEFAULT (unixepoch() * 1000),\n  UNIQUE(provider, provider_id)\n);\n```\n\n## Files\n- features/subscriptions/spec.md (Appendix C or new section §1.2.1)","status":"closed","priority":0,"issue_type":"task","created_at":"2025-12-16T19:34:13.228527-06:00","updated_at":"2025-12-16T19:39:37.294771-06:00","closed_at":"2025-12-16T19:39:37.294771-06:00","dependencies":[{"issue_id":"zine-n9v.2","depends_on_id":"zine-n9v","type":"parent-child","created_at":"2025-12-16T19:34:13.228859-06:00","created_by":"daemon"}]}
{"id":"zine-n9v.20","title":"Connect notification preferences UI to backend notification creation","description":"## Problem\nThe notification settings screen (§7.7.6) shows UI for preferences but the settings are local state only - they're not connected to the backend notification creation logic.\n\n## Context\n- §7.7.6 `NotificationSettingsScreen` uses `useState` for preferences\n- Comment says \"TODO: Persist to backend or local storage\"\n- §7.7.3 `createNotification` doesn't check user preferences\n- Users expect their settings to actually affect notifications\n- Also no way to fetch existing preferences\n\n## Solution\n1. Add a `user_notification_preferences` table in §7.7.2:\n\n```sql\nCREATE TABLE user_notification_preferences (\n  user_id TEXT PRIMARY KEY REFERENCES users(id) ON DELETE CASCADE,\n  connection_issues BOOLEAN NOT NULL DEFAULT TRUE,\n  subscription_errors BOOLEAN NOT NULL DEFAULT TRUE,\n  rate_limit_warnings BOOLEAN NOT NULL DEFAULT FALSE,\n  quota_status BOOLEAN NOT NULL DEFAULT TRUE,\n  push_enabled BOOLEAN NOT NULL DEFAULT FALSE,\n  updated_at INTEGER NOT NULL DEFAULT (unixepoch() * 1000)\n);\n```\n\n2. Add tRPC endpoints in §7.7.4:\n\n```typescript\nnotificationPreferences: router({\n  get: protectedProcedure.query(async ({ ctx }) =\u003e {\n    // Return preferences or defaults\n  }),\n  \n  update: protectedProcedure\n    .input(NotificationPreferencesSchema)\n    .mutation(async ({ ctx, input }) =\u003e {\n      // Upsert preferences\n    }),\n}),\n```\n\n3. Update §7.7.3 `createNotification` to check preferences:\n\n```typescript\nasync function createNotification(db: DrizzleDB, params: CreateNotificationParams) {\n  // Check user preferences\n  const prefs = await db.query.userNotificationPreferences.findFirst({\n    where: eq(userNotificationPreferences.userId, params.userId),\n  });\n  \n  const prefKey = getPreferenceKey(params.type);\n  if (prefs \u0026\u0026 prefs[prefKey] === false) {\n    // User has disabled this notification type\n    return null;\n  }\n  \n  // ... existing creation logic ...\n}\n```\n\n## Files\n- features/subscriptions/spec.md (§7.7.2, §7.7.3, §7.7.4, §7.7.6)","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-16T19:34:14.570134-06:00","updated_at":"2025-12-16T19:57:59.588804-06:00","closed_at":"2025-12-16T19:57:59.588804-06:00","dependencies":[{"issue_id":"zine-n9v.20","depends_on_id":"zine-n9v","type":"parent-child","created_at":"2025-12-16T19:34:14.570457-06:00","created_by":"daemon"}]}
{"id":"zine-n9v.21","title":"Add manual 'Sync Now' button feature to subscriptions UX","description":"## Problem\nUsers cannot manually trigger a sync for a specific subscription - they must wait for the next cron poll (up to 15+ minutes depending on adaptive interval).\n\n## Context\n- Polling intervals range from 1-24 hours (§3.3)\n- User subscribes to channel that just uploaded a video they want to see\n- No way to force immediate poll\n- Pull-to-refresh on subscription list only refetches local data, doesn't trigger server poll\n\n## Solution\nAdd a manual sync feature in §5.1:\n\n```typescript\n// tRPC endpoint\nsyncNow: protectedProcedure\n  .input(z.object({ subscriptionId: z.string() }))\n  .mutation(async ({ ctx, input }) =\u003e {\n    const subscription = await ctx.db.query.subscriptions.findFirst({\n      where: and(\n        eq(subscriptions.id, input.subscriptionId),\n        eq(subscriptions.userId, ctx.userId),\n        eq(subscriptions.status, 'ACTIVE')\n      ),\n    });\n    \n    if (!subscription) {\n      throw new TRPCError({ code: 'NOT_FOUND' });\n    }\n    \n    // Rate limit manual syncs (max 1 per 5 minutes per subscription)\n    const rateLimitKey = `manual-sync:${input.subscriptionId}`;\n    const lastSync = await ctx.env.KV.get(rateLimitKey);\n    if (lastSync \u0026\u0026 Date.now() - parseInt(lastSync) \u003c 5 * 60 * 1000) {\n      throw new TRPCError({\n        code: 'TOO_MANY_REQUESTS',\n        message: 'Please wait 5 minutes between manual syncs',\n      });\n    }\n    \n    // Perform sync\n    const connection = await getConnection(ctx.userId, subscription.provider, ctx.env);\n    const result = await pollSubscriptionWithRetry(subscription, connection, ctx.env);\n    \n    await ctx.env.KV.put(rateLimitKey, Date.now().toString(), { expirationTtl: 300 });\n    \n    return { \n      success: result.success, \n      itemsFound: result.itemsFound \n    };\n  }),\n```\n\nAdd UI button in §6.4 subscription detail or list item.\n\n## Files\n- features/subscriptions/spec.md (§5.1, §6.4)","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-16T19:34:14.643721-06:00","updated_at":"2025-12-16T20:07:07.888045-06:00","closed_at":"2025-12-16T20:07:07.888045-06:00","dependencies":[{"issue_id":"zine-n9v.21","depends_on_id":"zine-n9v","type":"parent-child","created_at":"2025-12-16T19:34:14.644079-06:00","created_by":"daemon"}]}
{"id":"zine-n9v.22","title":"Correct YouTube quota budget analysis with realistic polling frequencies","description":"Updated §3.6.3 with realistic adaptive polling capacity analysis","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-16T19:34:14.716998-06:00","updated_at":"2025-12-16T20:01:37.458366-06:00","closed_at":"2025-12-16T20:01:37.458366-06:00","dependencies":[{"issue_id":"zine-n9v.22","depends_on_id":"zine-n9v","type":"parent-child","created_at":"2025-12-16T19:34:14.717353-06:00","created_by":"daemon"}]}
{"id":"zine-n9v.23","title":"Add decryption error handling strategy (mark EXPIRED vs throw)","description":"## Problem\nWhen token decryption fails (wrong key, corrupted data), the spec doesn't specify the error handling strategy - should we mark the connection as EXPIRED or throw?\n\n## Context\n- §7.1 decrypt function throws on failure\n- §7.6.3 DecryptionError has different codes (KEY_VERSION_NOT_FOUND, INVALID_FORMAT, DECRYPTION_FAILED)\n- During polling, if decryption fails, we can't use the token\n- But \"wrong key\" might be transient (key rotation in progress) vs permanent (data corruption)\n- User experience differs: transient = retry later, permanent = reconnect required\n\n## Solution\nAdd a section §7.1.1 \"Decryption Error Handling\":\n\n```typescript\n/**\n * Handle decryption errors when accessing tokens.\n * \n * Strategy:\n * - KEY_VERSION_NOT_FOUND: Likely key rotation issue. Log error, \n *   don't mark expired, retry on next poll. Alert ops team.\n * - INVALID_FORMAT: Data corruption. Mark connection EXPIRED, \n *   notify user to reconnect.\n * - DECRYPTION_FAILED: Wrong key or corruption. If repeated (3x), \n *   mark EXPIRED. Otherwise retry.\n */\nasync function getTokenSafely(\n  connection: ProviderConnection,\n  env: Env\n): Promise\u003c{ token: string | null; error?: DecryptionError }\u003e {\n  try {\n    const token = await decrypt(connection.accessToken, env.ENCRYPTION_KEY);\n    return { token };\n  } catch (error) {\n    if (error instanceof DecryptionError) {\n      switch (error.code) {\n        case 'KEY_VERSION_NOT_FOUND':\n          // Likely a deployment issue - log and skip, don't fail permanently\n          console.error(`[Token] Key version not found for connection ${connection.id}`);\n          // TODO: Send ops alert\n          return { token: null, error };\n          \n        case 'INVALID_FORMAT':\n          // Data corruption - connection is unrecoverable\n          await markConnectionExpired(connection, 'decryption_failed');\n          return { token: null, error };\n          \n        case 'DECRYPTION_FAILED':\n          // Track failures, expire after threshold\n          const failureCount = await incrementDecryptionFailures(connection.id, env);\n          if (failureCount \u003e= 3) {\n            await markConnectionExpired(connection, 'decryption_failed');\n          }\n          return { token: null, error };\n      }\n    }\n    throw error;\n  }\n}\n```\n\n## Files\n- features/subscriptions/spec.md (new section §7.1.1 after line 6916)","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-16T19:34:14.797868-06:00","updated_at":"2025-12-16T19:58:00.485911-06:00","closed_at":"2025-12-16T19:58:00.485911-06:00","dependencies":[{"issue_id":"zine-n9v.23","depends_on_id":"zine-n9v","type":"parent-child","created_at":"2025-12-16T19:34:14.798222-06:00","created_by":"daemon"}]}
{"id":"zine-n9v.24","title":"Document D1 batch write limits and bulk ingestion strategy","description":"## Problem\nThe spec doesn't discuss D1's write limits or how to handle bulk ingestion when a subscription poll returns many new items.\n\n## Context\n- D1 has limits: 1000 rows per write, 100KB per row, transaction limits\n- A prolific YouTube channel might have 10+ new videos if user is late to subscribe\n- Initial fetch only gets latest item (§4.4), but backfill scenarios exist\n- Batch operations in §4.3 `ingestItem` are done one at a time\n\n## Solution\nAdd a section §4.3.1 \"Bulk Ingestion and D1 Limits\":\n\n```markdown\n### 4.3.1 Bulk Ingestion and D1 Limits\n\n#### D1 Write Limits (as of 2024)\n| Limit | Value |\n|-------|-------|\n| Max rows per write | 1,000 |\n| Max row size | 100 KB |\n| Max writes per transaction | 1,000 |\n| Recommended batch size | 100 rows |\n\n#### Handling Multiple New Items\n\nWhen a poll returns multiple new items (e.g., 10 new videos):\n\n```typescript\nasync function ingestItems(\n  userId: string,\n  subscriptionId: string,\n  items: ProviderItem[],\n  provider: Provider,\n  db: DrizzleDB\n): Promise\u003c{ created: number; skipped: number }\u003e {\n  // Process in batches to stay within D1 limits\n  const BATCH_SIZE = 25; // Conservative: 4 writes per item × 25 = 100 writes\n  \n  let created = 0;\n  let skipped = 0;\n  \n  for (let i = 0; i \u003c items.length; i += BATCH_SIZE) {\n    const batch = items.slice(i, i + BATCH_SIZE);\n    \n    // Process batch in a single transaction\n    await db.transaction(async (tx) =\u003e {\n      for (const item of batch) {\n        const result = await ingestItem(userId, subscriptionId, item, provider, tx);\n        if (result.created) created++;\n        else skipped++;\n      }\n    });\n  }\n  \n  return { created, skipped };\n}\n```\n\n#### Initial Subscription: Single Item Only\n\nAs specified in §4.4, initial subscription fetch intentionally returns only the \nlatest item to avoid bulk ingestion complexity. This design decision:\n- Keeps initial subscription fast\n- Avoids quota spikes\n- Prevents inbox overwhelming\n\nFor backfill scenarios (future feature), implement a separate background job \nwith aggressive rate limiting.\n```\n\n## Files\n- features/subscriptions/spec.md (new section §4.3.1 after line 3356)","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-16T19:34:14.880432-06:00","updated_at":"2025-12-16T19:58:01.507836-06:00","closed_at":"2025-12-16T19:58:01.507836-06:00","dependencies":[{"issue_id":"zine-n9v.24","depends_on_id":"zine-n9v","type":"parent-child","created_at":"2025-12-16T19:34:14.880761-06:00","created_by":"daemon"}]}
{"id":"zine-n9v.25","title":"Add React Error Boundary examples for mobile components","description":"## Problem\nThe mobile components in §6.6 don't show React Error Boundary usage. Unhandled errors in subscription UI could crash the entire app.\n\n## Context\n- OAuth callbacks can fail in various ways\n- Network errors during subscription operations\n- Malformed data from API responses\n- Without error boundaries, these crash the whole app\n- React Native apps should be resilient\n\n## Solution\nAdd error boundary examples in §6.5 and §6.6:\n\n```typescript\n// apps/mobile/components/error-boundary.tsx\n\nimport { Component, ErrorInfo, ReactNode } from 'react';\nimport { View, Text, Pressable } from 'react-native';\n\ninterface Props {\n  children: ReactNode;\n  fallback?: ReactNode;\n  onError?: (error: Error, errorInfo: ErrorInfo) =\u003e void;\n  resetKeys?: unknown[]; // Re-render children when these change\n}\n\ninterface State {\n  hasError: boolean;\n  error: Error | null;\n}\n\nexport class ErrorBoundary extends Component\u003cProps, State\u003e {\n  state: State = { hasError: false, error: null };\n\n  static getDerivedStateFromError(error: Error): State {\n    return { hasError: true, error };\n  }\n\n  componentDidCatch(error: Error, errorInfo: ErrorInfo) {\n    console.error('[ErrorBoundary]', error, errorInfo);\n    this.props.onError?.(error, errorInfo);\n    // TODO: Report to error tracking service\n  }\n\n  componentDidUpdate(prevProps: Props) {\n    if (this.state.hasError \u0026\u0026 this.props.resetKeys) {\n      const hasKeyChanged = this.props.resetKeys.some(\n        (key, i) =\u003e key !== prevProps.resetKeys?.[i]\n      );\n      if (hasKeyChanged) {\n        this.setState({ hasError: false, error: null });\n      }\n    }\n  }\n\n  render() {\n    if (this.state.hasError) {\n      return this.props.fallback || (\n        \u003cView className=\"flex-1 items-center justify-center p-6\"\u003e\n          \u003cText className=\"text-xl mb-2\"\u003eSomething went wrong\u003c/Text\u003e\n          \u003cText className=\"text-gray-500 mb-4 text-center\"\u003e\n            {this.state.error?.message || 'An unexpected error occurred'}\n          \u003c/Text\u003e\n          \u003cPressable\n            onPress={() =\u003e this.setState({ hasError: false, error: null })}\n            className=\"bg-blue-500 px-6 py-3 rounded-lg\"\n          \u003e\n            \u003cText className=\"text-white font-medium\"\u003eTry Again\u003c/Text\u003e\n          \u003c/Pressable\u003e\n        \u003c/View\u003e\n      );\n    }\n\n    return this.props.children;\n  }\n}\n\n// Usage in subscription screens:\n\u003cErrorBoundary \n  onError={(e) =\u003e trackError('subscription_screen', e)}\n  resetKeys={[subscriptionId]}\n\u003e\n  \u003cSubscriptionDetail id={subscriptionId} /\u003e\n\u003c/ErrorBoundary\u003e\n```\n\nAdd this pattern around OAuth screens, subscription list, and notification components.\n\n## Files\n- features/subscriptions/spec.md (§6.5, §6.6)","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-16T19:34:14.962042-06:00","updated_at":"2025-12-16T20:01:38.706117-06:00","closed_at":"2025-12-16T20:01:38.706117-06:00","dependencies":[{"issue_id":"zine-n9v.25","depends_on_id":"zine-n9v","type":"parent-child","created_at":"2025-12-16T19:34:14.962381-06:00","created_by":"daemon"}]}
{"id":"zine-n9v.3","title":"Add user_items table schema to spec (or reference existing schema)","description":"## Problem\nThe `user_items` table is a critical junction table between users and items (storing INBOX/BOOKMARKED/ARCHIVED state) but its schema is never defined in the spec.\n\n## Context\n- §1.2 shows `user_items` in the entity relationship: `users (N) user_items (N) items`\n- §4.3 ingestion code creates user_items with `state: 'INBOX'`\n- §5.3 unsubscribe behavior deletes `user_items` with `state='INBOX'`\n- The table is central to the inbox-first flow described in §1 Key Principles\n\n## Solution\nAdd the schema to Appendix C or reference existing schema:\n\n```sql\nCREATE TABLE user_items (\n  id TEXT PRIMARY KEY,\n  user_id TEXT NOT NULL REFERENCES users(id) ON DELETE CASCADE,\n  item_id TEXT NOT NULL REFERENCES items(id),\n  state TEXT NOT NULL DEFAULT 'INBOX',  -- INBOX, BOOKMARKED, ARCHIVED\n  ingested_at INTEGER NOT NULL DEFAULT (unixepoch() * 1000),\n  bookmarked_at INTEGER,\n  archived_at INTEGER,\n  updated_at INTEGER NOT NULL DEFAULT (unixepoch() * 1000),\n  UNIQUE(user_id, item_id)\n);\n\nCREATE INDEX idx_user_items_inbox ON user_items(user_id, state);\nCREATE INDEX idx_user_items_item ON user_items(item_id);\n```\n\n## Files\n- features/subscriptions/spec.md (Appendix C)","status":"closed","priority":0,"issue_type":"task","created_at":"2025-12-16T19:34:13.303513-06:00","updated_at":"2025-12-16T19:39:37.937897-06:00","closed_at":"2025-12-16T19:39:37.937897-06:00","dependencies":[{"issue_id":"zine-n9v.3","depends_on_id":"zine-n9v","type":"parent-child","created_at":"2025-12-16T19:34:13.303843-06:00","created_by":"daemon"}]}
{"id":"zine-n9v.4","title":"Clarify timestamp convention: milliseconds vs seconds across all tables","description":"## Problem\nThe spec inconsistently uses milliseconds vs seconds for timestamps, which could cause bugs during implementation.\n\n## Context\n- SQL defaults use `unixepoch() * 1000` (milliseconds): lines 9421, 9441, 9452\n- §3.2 polling logic: `lastPolledAt \u003c now - (pollIntervalSeconds * 1000)` - explicitly converts\n- §3.4 token expiry: `now + refreshed.expiresIn * 1000` - explicitly converts\n- But `poll_interval_seconds` column stores seconds (line 9438), requiring conversion\n- `token_expires_at` stores milliseconds (line 9420)\n\n## Solution\nAdd a convention note in §1.2 after the table definitions:\n\n```markdown\n### 1.2.1 Timestamp Convention\n\nAll timestamp columns in the database store **Unix timestamps in milliseconds** (not seconds).\n\n| Column Type | Storage | Example |\n|-------------|---------|---------|\n| `*_at` columns | Milliseconds | `1702747200000` |\n| `*_seconds` columns | Seconds | `3600` |\n\nWhen working with external APIs:\n- JavaScript `Date.now()` returns milliseconds (use directly)\n- Provider API responses often use ISO 8601 strings (parse with `new Date().getTime()`)\n- OAuth `expires_in` is in seconds (multiply by 1000 before storing)\n```\n\n## Files\n- features/subscriptions/spec.md (new section §1.2.1)","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-16T19:34:13.379647-06:00","updated_at":"2025-12-16T19:42:01.765492-06:00","closed_at":"2025-12-16T19:42:01.765492-06:00","dependencies":[{"issue_id":"zine-n9v.4","depends_on_id":"zine-n9v","type":"parent-child","created_at":"2025-12-16T19:34:13.379981-06:00","created_by":"daemon"}]}
{"id":"zine-n9v.5","title":"Add concurrent token refresh handling with locking mechanism","description":"## Problem\nMultiple cron jobs or parallel polling workers could attempt to refresh the same expired OAuth token simultaneously, leading to race conditions where one refresh invalidates another's token.\n\n## Context\n- §3.4 `getValidToken()` checks expiry and refreshes if needed\n- Multiple subscriptions for the same user could be polled concurrently\n- Spotify rotates refresh tokens on each use - if two workers refresh simultaneously, one will have an invalid refresh token\n- No locking mechanism is described\n\n## Solution\nAdd a section §3.4.1 \"Concurrent Token Refresh\" with a distributed locking strategy using KV:\n\n```typescript\n// apps/worker/src/lib/tokens.ts\n\nconst REFRESH_LOCK_TTL = 30; // seconds\n\nasync function getValidTokenWithLock(\n  connection: ProviderConnection,\n  env: Env\n): Promise\u003cstring\u003e {\n  const now = Date.now();\n  const bufferMs = 5 * 60 * 1000;\n  \n  // Return existing token if still valid\n  if (connection.tokenExpiresAt - bufferMs \u003e now) {\n    return decrypt(connection.accessToken, env.ENCRYPTION_KEY);\n  }\n  \n  // Try to acquire refresh lock\n  const lockKey = `token:refresh:${connection.id}`;\n  const lockAcquired = await tryAcquireLock(env.KV, lockKey, REFRESH_LOCK_TTL);\n  \n  if (!lockAcquired) {\n    // Another worker is refreshing - wait and retry\n    await sleep(2000);\n    // Re-fetch connection to get updated token\n    const updated = await getConnection(connection.id, env);\n    if (updated \u0026\u0026 updated.tokenExpiresAt - bufferMs \u003e now) {\n      return decrypt(updated.accessToken, env.ENCRYPTION_KEY);\n    }\n    throw new Error('Token refresh in progress by another worker');\n  }\n  \n  try {\n    const refreshed = await refreshToken(connection, env);\n    await persistTokens(connection.id, refreshed, env);\n    return refreshed.accessToken;\n  } finally {\n    await releaseLock(env.KV, lockKey);\n  }\n}\n\nasync function tryAcquireLock(kv: KVNamespace, key: string, ttl: number): Promise\u003cboolean\u003e {\n  const existing = await kv.get(key);\n  if (existing) return false;\n  \n  // Use put with expiration - not perfectly atomic but good enough for this use case\n  await kv.put(key, Date.now().toString(), { expirationTtl: ttl });\n  return true;\n}\n```\n\n## Files\n- features/subscriptions/spec.md (new section §3.4.1 after line 1176)","status":"closed","priority":0,"issue_type":"task","created_at":"2025-12-16T19:34:13.453285-06:00","updated_at":"2025-12-16T19:40:08.830606-06:00","closed_at":"2025-12-16T19:40:08.830606-06:00","dependencies":[{"issue_id":"zine-n9v.5","depends_on_id":"zine-n9v","type":"parent-child","created_at":"2025-12-16T19:34:13.453649-06:00","created_by":"daemon"}]}
{"id":"zine-n9v.6","title":"Add cron job collision handling (mutex/distributed lock)","description":"## Problem\nThe 15-minute cron interval (§3.2) could lead to overlapping executions if a batch takes longer than 15 minutes to complete, causing duplicate processing.\n\n## Context\n- Cron trigger runs every 15 minutes (line 2925)\n- Batch size is 50 subscriptions\n- Each subscription requires API calls that may be rate-limited\n- Cloudflare Workers have a 30-second CPU time limit, but cron handlers can run longer with `ctx.waitUntil()`\n- No mechanism prevents the next cron from starting while previous is still running\n\n## Solution\nAdd a section §3.2.1 \"Cron Job Collision Prevention\":\n\n```typescript\n// apps/worker/src/jobs/poll-subscriptions.ts\n\nconst POLL_LOCK_KEY = 'cron:poll-subscriptions:lock';\nconst POLL_LOCK_TTL = 900; // 15 minutes (matches cron interval)\n\nexport async function pollSubscriptions(env: Env) {\n  // Check for existing execution\n  const existingLock = await env.KV.get(POLL_LOCK_KEY);\n  if (existingLock) {\n    const lockTime = parseInt(existingLock, 10);\n    const elapsed = Date.now() - lockTime;\n    \n    // If lock is recent, skip this execution\n    if (elapsed \u003c POLL_LOCK_TTL * 1000) {\n      console.log(`[Poll] Skipping - previous execution still running (${Math.round(elapsed/1000)}s elapsed)`);\n      return;\n    }\n    // Stale lock - previous execution may have crashed\n    console.warn(`[Poll] Stale lock detected (${Math.round(elapsed/1000)}s old), proceeding`);\n  }\n  \n  // Acquire lock\n  await env.KV.put(POLL_LOCK_KEY, Date.now().toString(), { \n    expirationTtl: POLL_LOCK_TTL \n  });\n  \n  try {\n    // ... existing polling logic ...\n  } finally {\n    // Release lock on completion\n    await env.KV.delete(POLL_LOCK_KEY);\n  }\n}\n```\n\nAlso mention that the batch size (50) was chosen to typically complete within 10 minutes, leaving buffer before next cron.\n\n## Files\n- features/subscriptions/spec.md (new section §3.2.1 after line 2969)","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-16T19:34:13.523751-06:00","updated_at":"2025-12-16T19:42:45.623491-06:00","closed_at":"2025-12-16T19:42:45.623491-06:00","dependencies":[{"issue_id":"zine-n9v.6","depends_on_id":"zine-n9v","type":"parent-child","created_at":"2025-12-16T19:34:13.524056-06:00","created_by":"daemon"}]}
{"id":"zine-n9v.7","title":"Fix Web Crypto migration: handle 16-byte to 12-byte IV format change","description":"## Problem\nThe spec notes a format change between Node.js crypto and Web Crypto API (§7.1 note around line 6920) but the migration function in §7.6.4 doesn't handle decrypting legacy tokens encrypted with the old format.\n\n## Context\n- Old format (Node.js): `iv:authTag:ciphertext` with 16-byte IV\n- New format (Web Crypto): `iv:ciphertext` with 12-byte IV (auth tag appended to ciphertext)\n- §7.6.3 `parseVersionedCiphertext` handles versioned (`v1:...`) and legacy (no version prefix) formats\n- But the legacy format parsing assumes the NEW Web Crypto format, not the OLD Node.js format\n- Existing tokens encrypted with Node.js would fail to decrypt\n\n## Solution\nUpdate §7.6.3 `parseVersionedCiphertext` to detect and handle both legacy formats:\n\n```typescript\nfunction parseVersionedCiphertext(encrypted: string, env: Env): ParsedCiphertext {\n  // New versioned format: v{n}:{iv}:{authTag}:{ciphertext}\n  if (encrypted.startsWith('v')) {\n    // ... existing versioned handling ...\n  }\n  \n  // Legacy format detection\n  const parts = encrypted.split(':');\n  \n  // Old Node.js format: {iv}:{authTag}:{ciphertext} (3 parts, 16-byte IV)\n  if (parts.length === 3) {\n    const iv = hexToBytes(parts[0]);\n    \n    if (iv.length === 16) {\n      // Old Node.js format with 16-byte IV\n      return {\n        version: 0, // Special version for legacy Node.js format\n        iv,\n        authTag: hexToBytes(parts[1]),\n        ciphertext: hexToBytes(parts[2]),\n        format: 'nodejs'\n      };\n    }\n    \n    // New Web Crypto legacy format with 12-byte IV\n    if (iv.length === 12) {\n      return {\n        version: 1,\n        iv,\n        authTag: hexToBytes(parts[1]),\n        ciphertext: hexToBytes(parts[2]),\n        format: 'webcrypto'\n      };\n    }\n  }\n  \n  // New Web Crypto format: {iv}:{ciphertext} (2 parts)\n  if (parts.length === 2) {\n    // ... existing handling ...\n  }\n  \n  throw new DecryptionError('Invalid ciphertext format', 'INVALID_FORMAT');\n}\n```\n\nThen update the decrypt function to use appropriate algorithm parameters based on format.\n\n## Files\n- features/subscriptions/spec.md (§7.6.3 around line 7413)","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-16T19:34:13.59684-06:00","updated_at":"2025-12-16T19:57:56.2399-06:00","closed_at":"2025-12-16T19:57:56.2399-06:00","dependencies":[{"issue_id":"zine-n9v.7","depends_on_id":"zine-n9v","type":"parent-child","created_at":"2025-12-16T19:34:13.597161-06:00","created_by":"daemon"}]}
{"id":"zine-n9v.8","title":"Add transaction boundaries to ingestion pipeline (§4.3)","description":"## Problem\nThe ingestion pipeline (§4.3) performs multiple database operations that should be atomic but aren't wrapped in a transaction. Partial failures could leave orphaned or inconsistent records.\n\n## Context\nThe `ingestItem` function (lines 3289-3356) does:\n1. Check `provider_items_seen` (read)\n2. Find or create `items` (read/write)\n3. Create `user_items` (write)\n4. Create `subscription_items` (write)\n5. Create `provider_items_seen` (write)\n\nIf step 4 fails after step 3 succeeds:\n- User sees item in inbox\n- But no record linking it to subscription\n- `provider_items_seen` not created → could be re-ingested\n\n## Solution\nUpdate §4.3 to wrap operations in a transaction. D1 supports transactions via the batch API:\n\n```typescript\nasync function ingestItem(\n  userId: string,\n  subscriptionId: string,\n  rawItem: YouTubeVideo | SpotifyEpisode,\n  provider: Provider,\n  db: DrizzleDB\n): Promise\u003c{ created: boolean; userItemId: string }\u003e {\n  const providerId = rawItem.id;\n  \n  // 1. Check idempotency (read - outside transaction is fine)\n  const seen = await db.query.providerItemsSeen.findFirst({...});\n  if (seen) {\n    return { created: false, userItemId: seen.userItemId };\n  }\n  \n  // 2-5. Atomic writes using transaction\n  return await db.transaction(async (tx) =\u003e {\n    // Find or create canonical item\n    let item = await tx.query.items.findFirst({...});\n    if (!item) {\n      const transformed = transformItem(rawItem, provider);\n      [item] = await tx.insert(items).values(transformed).returning();\n    }\n    \n    // Create user_item\n    const userItemId = ulid();\n    await tx.insert(userItems).values({\n      id: userItemId,\n      userId,\n      itemId: item.id,\n      state: 'INBOX',\n      ingestedAt: Date.now(),\n    });\n    \n    // Create subscription_item\n    await tx.insert(subscriptionItems).values({...});\n    \n    // Mark as seen (idempotency)\n    await tx.insert(providerItemsSeen).values({\n      id: ulid(),\n      userId,\n      provider,\n      providerItemId: providerId,\n      userItemId,\n      firstSeenAt: Date.now(),\n    });\n    \n    return { created: true, userItemId };\n  });\n}\n```\n\nNote: D1's transaction support may have limits - add a note about checking current D1 transaction capabilities.\n\n## Files\n- features/subscriptions/spec.md (§4.3 around line 3289)","status":"closed","priority":0,"issue_type":"task","created_at":"2025-12-16T19:34:13.668451-06:00","updated_at":"2025-12-16T19:40:41.970397-06:00","closed_at":"2025-12-16T19:40:41.970397-06:00","dependencies":[{"issue_id":"zine-n9v.8","depends_on_id":"zine-n9v","type":"parent-child","created_at":"2025-12-16T19:34:13.668763-06:00","created_by":"daemon"}]}
{"id":"zine-n9v.9","title":"Add race condition handling for canonical item creation","description":"## Problem\nWhen two users subscribe to the same channel simultaneously, the `findOrCreateCanonicalItem` logic could race, potentially creating duplicate canonical items or failing on unique constraint.\n\n## Context\n- §4.3 line 3312 does `findFirst` then `insert` if not found\n- Two workers polling different users' subscriptions to the same channel at the same time\n- Both could find no existing item, both try to insert → one fails with unique constraint violation\n- The `items` table has `UNIQUE(provider, provider_id)` but error handling isn't shown\n\n## Solution\nAdd a section §4.2.1 \"Canonical Item Upsert\" with proper conflict handling:\n\n```typescript\n/**\n * Find or create a canonical item using upsert pattern.\n * Handles race conditions from concurrent ingestion.\n */\nasync function findOrCreateCanonicalItem(\n  provider: Provider,\n  providerId: string,\n  transformFn: () =\u003e NewItem,\n  db: DrizzleDB\n): Promise\u003cItem\u003e {\n  // First, try to find existing\n  const existing = await db.query.items.findFirst({\n    where: and(\n      eq(items.provider, provider),\n      eq(items.providerId, providerId)\n    ),\n  });\n  \n  if (existing) {\n    return existing;\n  }\n  \n  // Not found - try to insert with conflict handling\n  try {\n    const newItem = transformFn();\n    const [created] = await db\n      .insert(items)\n      .values(newItem)\n      .onConflictDoNothing({ target: [items.provider, items.providerId] })\n      .returning();\n    \n    if (created) {\n      return created;\n    }\n    \n    // Conflict occurred - another worker created it, fetch it\n    const raceWinner = await db.query.items.findFirst({\n      where: and(\n        eq(items.provider, provider),\n        eq(items.providerId, providerId)\n      ),\n    });\n    \n    if (!raceWinner) {\n      throw new Error(`Item disappeared after conflict: ${provider}/${providerId}`);\n    }\n    \n    return raceWinner;\n  } catch (error) {\n    // Handle any other constraint violations\n    if (isUniqueConstraintError(error)) {\n      // Race condition - fetch the winner\n      const existing = await db.query.items.findFirst({...});\n      if (existing) return existing;\n    }\n    throw error;\n  }\n}\n```\n\n## Files\n- features/subscriptions/spec.md (new section §4.2.1 after line 3284)","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-16T19:34:13.746935-06:00","updated_at":"2025-12-16T19:43:18.9321-06:00","closed_at":"2025-12-16T19:43:18.9321-06:00","dependencies":[{"issue_id":"zine-n9v.9","depends_on_id":"zine-n9v","type":"parent-child","created_at":"2025-12-16T19:34:13.747298-06:00","created_by":"daemon"}]}
